{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to text analysis VII #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances and similarities among strings ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will study the techniques for studying the similarity of strings. To measure the similary of two strings there are several methods. The most famous one is the *Levenshtein distance*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Levenshtein or edit distance is the number of characters that need to be substituted, inserted, or deleted, to transform s1 into s2. For example, transforming “rain” to “shine” requires three steps, consisting of two substitutions and one insertion: “rain” -> “sain” -> “shin” -> “shine”. These operations could have been done in other orders, but at least three steps are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** edit distance **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "s1 = \"The pen is on the table\"\n",
    "s2 = \"The cat is on the table\"\n",
    "s3 = \"This is a more different string !\"\n",
    "\n",
    "d = edit_distance(s1,s2)\n",
    "\n",
    "print(d)\n",
    "\n",
    "\n",
    "d1 = edit_distance(s1,s3)\n",
    "\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"mistake\"\n",
    "b = \"mestake\"\n",
    "edit_distance(a,b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** confronting sets with the jaccard distance **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'pen', 'on', 'the', 'The', 'table'}\n",
      "{'cat', 'is', 'on', 'the', 'The', 'table'}\n",
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import jaccard_distance\n",
    "\n",
    "\n",
    "tokens1 = set(s1.split())\n",
    "tokens2 = set(s2.split())\n",
    "tokens3 = set(s3.split())\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "d = jaccard_distance(tokens1,tokens3)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency, inverse document frequency ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of a term computed with the *FreqDist* function is a first approximation of importance of a word in a text. However the most common terms such as *the* tends to be too much represented. To improve the evaluation we build the inverse document metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a corpus of *D* documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "D = {d_1, d_2, ... , d_n}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the frequency of each word is weighted against the importance of each document with the *inverse document frequency*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "idf(t,d) = log \\frac{N_D}{N_{(t,D)}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $N_D$ is the number of documents, $N_{(t,D)}$ is the number of documents where the word $t$ appears. As a word can be absent from all documents a case similar makes a division by zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we weight the importance of a term for the number of times a word is appearing in the documents words such as *the* will tend to disappear (as they will appear in all documents), then the complete tf-idf formula can be expressed as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the product of the term frequency $tf(t,d)$ in the document $d$ per the frequency of the term $t$ in all the corpus $D$.The formula above has several versions with minor adjustements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python a high level library for machine learning and text analysis is **scikit-learn** here we leverage the functions for text analysis and in particular tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "def removePunct(text):\n",
    "    nt = []\n",
    "    for t in text:\n",
    "        if(t not in string.punctuation):\n",
    "            nt.append(t)\n",
    "        else:\n",
    "            nt.append(\" \")\n",
    "    return \"\".join(nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvin = \"inaug_speeches.csv\"\n",
    "\n",
    "documents = []\n",
    "with codecs.open(csvin,\"r\",\"iso-8859-2\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        text = row['text']\n",
    "        text = text.lower()\n",
    "        \n",
    "        no_punctuation = removePunct(text)\n",
    "        documents.append(no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<58x5296 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 32347 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text it hasn't seen gets excluded, unless you call fit_and_transform(), which adds the document to the model, whereas transform does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4636)\t0.127775908687\n",
      "  (0, 4438)\t0.383084428239\n",
      "  (0, 2088)\t0.127775908687\n",
      "  (0, 1726)\t0.148924666465\n",
      "  (0, 1500)\t0.443139569052\n",
      "  (0, 1422)\t0.159578827315\n",
      "  (0, 1097)\t0.354572058202\n",
      "  (0, 906)\t0.242708244759\n",
      "  (0, 811)\t0.27946708298\n",
      "  (0, 368)\t0.560219449939\n",
      "  (1, 4636)\t0.130508325523\n",
      "  (1, 4438)\t0.391276475956\n",
      "  (1, 3329)\t0.135009058615\n",
      "  (1, 2088)\t0.130508325523\n",
      "  (1, 1726)\t0.152109337738\n",
      "  (1, 1500)\t0.452615862598\n",
      "  (1, 1422)\t0.1629913319\n",
      "  (1, 1097)\t0.362154384723\n",
      "  (1, 811)\t0.285443331322\n",
      "  (1, 368)\t0.572199431707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#newstring = 'this sentence has unseen text such as computer but also king lord juliet'\n",
    "newstring = \"In this conflict of emotions all I dare aver is that it has been my faithful study to collect my duty\"\n",
    "#newstring = 'sentence'\n",
    "newstring2 = \"In this people some of emotions all I dare aver is that it has been my faithful study to collect my duty\"\n",
    "#n\n",
    "response = tfidf.transform([newstring,newstring2])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** associating words and tf-idf frequencies **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thi  -  0.127775908687\n",
      "studi  -  0.383084428239\n",
      "ha  -  0.127775908687\n",
      "faith  -  0.148924666465\n",
      "emot  -  0.443139569052\n",
      "duti  -  0.159578827315\n",
      "dare  -  0.354572058202\n",
      "conflict  -  0.242708244759\n",
      "collect  -  0.27946708298\n",
      "aver  -  0.560219449939\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for col in response.nonzero()[1]:\n",
    "    print (feature_names[col], ' - ', response[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
